{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a GUI demo built with [gradio](https://gradio.app/), **more powerful** than our [HuggingFace Demo ü§ó](https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text), as this demo outputs not only tags and/or captions, but also **boxes** and **masks** from [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our code\n",
    "%pip install -r requirements.txt\n",
    "%pip install -e .\n",
    "\n",
    "# install gradio\n",
    "%pip install --upgrade gradio\n",
    "\n",
    "# set up Grounded-SAM\n",
    "# you will need to clone Grounded-SAM repo\n",
    "%git clone https://github.com/IDEA-Research/Grounded-Segment-Anything.git\n",
    "%cd ./Grounded-Segment-Anything\n",
    "%pip install -r ./requirements.txt\n",
    "%pip install ./segment_anything\n",
    "%pip install ./GroundingDINO\n",
    "%cd ..\n",
    "%pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments.\n",
    "#\n",
    "# before you go, please download following 4 checkpoints:\n",
    "# download RAM and Tag2Text checkpoints to ./pretrained/ from https://github.com/majinyu666/recognize-anything/tree/main#toolbox-checkpoints\n",
    "# download GroundingDINO and SAM checkpoints to ./Grounded-Segment-Anything/ from step 1 of https://github.com/IDEA-Research/Grounded-Segment-Anything#running_man-grounded-sam-detect-and-segment-everything-with-text-prompt\n",
    "config_file = \"./Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "ram_checkpoint = \"./pretrained/ram_swin_large_14m.pth\"\n",
    "tag2text_checkpoint = \"./pretrained/tag2text_swin_14m.pth\"\n",
    "grounded_checkpoint = \"./Grounded-Segment-Anything/groundingdino_swint_ogc.pth\"\n",
    "sam_checkpoint = \"./Grounded-Segment-Anything/sam_vit_h_4b8939.pth\"\n",
    "box_threshold = 0.25\n",
    "text_threshold = 0.2\n",
    "iou_threshold = 0.5\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading, inference and visualization functions\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import groundingdino.datasets.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as TS\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ram import inference_ram\n",
    "from ram import inference_tag2text\n",
    "from ram.models import ram\n",
    "from ram.models import tag2text_caption\n",
    "from segment_anything import SamPredictor, build_sam\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(\n",
    "        clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold, device=\"cpu\"):\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"].cpu()[0]  # (nq, 4)\n",
    "    logits.shape[0]\n",
    "\n",
    "    # filter output\n",
    "    logits_filt = logits.clone()\n",
    "    boxes_filt = boxes.clone()\n",
    "    filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "    logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "    boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "    logits_filt.shape[0]\n",
    "\n",
    "    # get phrase\n",
    "    tokenlizer = model.tokenizer\n",
    "    tokenized = tokenlizer(caption)\n",
    "    # build pred\n",
    "    pred_phrases = []\n",
    "    scores = []\n",
    "    for logit, box in zip(logits_filt, boxes_filt):\n",
    "        pred_phrase = get_phrases_from_posmap(\n",
    "            logit > text_threshold, tokenized, tokenlizer)\n",
    "        pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "        scores.append(logit.max().item())\n",
    "\n",
    "    return boxes_filt, torch.Tensor(scores), pred_phrases\n",
    "\n",
    "\n",
    "def draw_mask(mask, draw, random_color=False):\n",
    "    if random_color:\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255), 153)\n",
    "    else:\n",
    "        color = (30, 144, 255, 153)\n",
    "\n",
    "    nonzero_coords = np.transpose(np.nonzero(mask))\n",
    "\n",
    "    for coord in nonzero_coords:\n",
    "        draw.point(coord[::-1], fill=color)\n",
    "\n",
    "\n",
    "def draw_box(box, draw, label):\n",
    "    # random color\n",
    "    color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "    line_width = int(max(4, min(20, 0.006*max(draw.im.size))))\n",
    "    draw.rectangle(((box[0], box[1]), (box[2], box[3])), outline=color,  width=line_width)\n",
    "\n",
    "    if label:\n",
    "        font_path = os.path.join(\n",
    "            cv2.__path__[0], 'qt', 'fonts', 'DejaVuSans.ttf')\n",
    "        font_size = int(max(12, min(60, 0.02*max(draw.im.size))))\n",
    "        font = ImageFont.truetype(font_path, size=font_size)\n",
    "        if hasattr(font, \"getbbox\"):\n",
    "            bbox = draw.textbbox((box[0], box[1]), str(label), font)\n",
    "        else:\n",
    "            w, h = draw.textsize(str(label), font)\n",
    "            bbox = (box[0], box[1], w + box[0], box[1] + h)\n",
    "        draw.rectangle(bbox, fill=color)\n",
    "        draw.text((box[0], box[1]), str(label), fill=\"white\", font=font)\n",
    "\n",
    "        draw.text((box[0], box[1]), label, font=font)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(\n",
    "    raw_image, specified_tags, do_det_seg,\n",
    "    tagging_model_type, tagging_model, grounding_dino_model, sam_model\n",
    "):\n",
    "    print(f\"Start processing, image size {raw_image.size}\")\n",
    "    raw_image = raw_image.convert(\"RGB\")\n",
    "\n",
    "    # run tagging model\n",
    "    normalize = TS.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transform = TS.Compose([\n",
    "        TS.Resize((384, 384)),\n",
    "        TS.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    image = raw_image.resize((384, 384))\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Currently \", \" is better for detecting single tags\n",
    "    # while \". \" is a little worse in some case\n",
    "    if tagging_model_type == \"RAM\":\n",
    "        res = inference_ram(image, tagging_model)\n",
    "        tags = res[0].strip(' ').replace('  ', ' ').replace(' |', ',')\n",
    "        tags_chinese = res[1].strip(' ').replace('  ', ' ').replace(' |', ',')\n",
    "        print(\"Tags: \", tags)\n",
    "        print(\"ÂõæÂÉèÊ†áÁ≠æ: \", tags_chinese)\n",
    "    else:\n",
    "        res = inference_tag2text(image, tagging_model, specified_tags)\n",
    "        tags = res[0].strip(' ').replace('  ', ' ').replace(' |', ',')\n",
    "        caption = res[2]\n",
    "        print(f\"Tags: {tags}\")\n",
    "        print(f\"Caption: {caption}\")\n",
    "\n",
    "    # return\n",
    "    if not do_det_seg:\n",
    "        if tagging_model_type == \"RAM\":\n",
    "            return tags.replace(\", \", \" | \"), tags_chinese.replace(\", \", \" | \"), None\n",
    "        else:\n",
    "            return tags.replace(\", \", \" | \"), caption, None\n",
    "\n",
    "    # run groundingDINO\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image, _ = transform(raw_image, None)  # 3, h, w\n",
    "\n",
    "    boxes_filt, scores, pred_phrases = get_grounding_output(\n",
    "        grounding_dino_model, image, tags, box_threshold, text_threshold, device=device\n",
    "    )\n",
    "    print(\"GroundingDINO finished\")\n",
    "\n",
    "    # run SAM\n",
    "    image = np.asarray(raw_image)\n",
    "    sam_model.set_image(image)\n",
    "\n",
    "    size = raw_image.size\n",
    "    H, W = size[1], size[0]\n",
    "    for i in range(boxes_filt.size(0)):\n",
    "        boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])\n",
    "        boxes_filt[i][:2] -= boxes_filt[i][2:] / 2\n",
    "        boxes_filt[i][2:] += boxes_filt[i][:2]\n",
    "\n",
    "    boxes_filt = boxes_filt.cpu()\n",
    "    # use NMS to handle overlapped boxes\n",
    "    print(f\"Before NMS: {boxes_filt.shape[0]} boxes\")\n",
    "    nms_idx = torchvision.ops.nms(boxes_filt, scores, iou_threshold).numpy().tolist()\n",
    "    boxes_filt = boxes_filt[nms_idx]\n",
    "    pred_phrases = [pred_phrases[idx] for idx in nms_idx]\n",
    "    print(f\"After NMS: {boxes_filt.shape[0]} boxes\")\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_filt, image.shape[:2]).to(device)\n",
    "\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes.to(device),\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    print(\"SAM finished\")\n",
    "\n",
    "    # draw output image\n",
    "    mask_image = Image.new('RGBA', size, color=(0, 0, 0, 0))\n",
    "\n",
    "    mask_draw = ImageDraw.Draw(mask_image)\n",
    "    for mask in masks:\n",
    "        draw_mask(mask[0].cpu().numpy(), mask_draw, random_color=True)\n",
    "\n",
    "    image_draw = ImageDraw.Draw(raw_image)\n",
    "\n",
    "    for box, label in zip(boxes_filt, pred_phrases):\n",
    "        draw_box(box, image_draw, label)\n",
    "\n",
    "    out_image = raw_image.convert('RGBA')\n",
    "    out_image.alpha_composite(mask_image)\n",
    "\n",
    "    # return\n",
    "    if tagging_model_type == \"RAM\":\n",
    "        return tags.replace(\", \", \" | \"), tags_chinese.replace(\", \", \" | \"), out_image\n",
    "    else:\n",
    "        return tags.replace(\", \", \" | \"), caption, out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 4 models\n",
    "\n",
    "# load RAM\n",
    "ram_model = ram(pretrained=ram_checkpoint, image_size=384, vit='swin_l')\n",
    "ram_model.eval()\n",
    "ram_model = ram_model.to(device)\n",
    "\n",
    "# load Tag2Text\n",
    "delete_tag_index = []  # filter out attributes and action categories which are difficult to grounding\n",
    "for i in range(3012, 3429):\n",
    "    delete_tag_index.append(i)\n",
    "\n",
    "tag2text_model = tag2text_caption(pretrained=tag2text_checkpoint,\n",
    "                                    image_size=384,\n",
    "                                    vit='swin_b',\n",
    "                                    delete_tag_index=delete_tag_index)\n",
    "tag2text_model.threshold = 0.64  # we reduce the threshold to obtain more tags\n",
    "tag2text_model.eval()\n",
    "tag2text_model = tag2text_model.to(device)\n",
    "\n",
    "# load groundingDINO\n",
    "grounding_dino_model = load_model(config_file, grounded_checkpoint, device=device)\n",
    "\n",
    "# load SAM\n",
    "sam_model = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build GUI\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def build_gui():\n",
    "\n",
    "    description = \"\"\"\n",
    "        <center><strong><font size='10'>Recognize Anything Model + Grounded-SAM</font></strong></center>\n",
    "        <br>\n",
    "        Welcome to the RAM/Tag2Text + Grounded-SAM demo! <br><br>\n",
    "        <li>\n",
    "            <b>Recognize Anything Model:</b> Upload your image to get the <b>English and Chinese tags</b>!\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>Tag2Text Model:</b> Upload your image to get the <b>tags and caption</b>!\n",
    "            (Optional: Specify tags to get the corresponding caption.)\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>Grounded-SAM:</b> Tick the checkbox to get <b>boxes</b> and <b>masks</b> of tags!\n",
    "        </li>\n",
    "        <br>\n",
    "        Great thanks to <a href='https://huggingface.co/majinyu' target='_blank'>Ma Jinyu</a>, the major contributor of this demo!\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    article = \"\"\"\n",
    "        <p style='text-align: center'>\n",
    "            RAM and Tag2Text are trained on open-source datasets, and we are persisting in refining and iterating upon it.<br/>\n",
    "            Grounded-SAM is a combination of Grounding DINO and SAM aming to detect and segment anything with text inputs.<br/>\n",
    "            <a href='https://recognize-anything.github.io/' target='_blank'>Recognize Anything: A Strong Image Tagging Model</a>\n",
    "            |\n",
    "            <a href='https://https://tag2text.github.io/' target='_blank'>Tag2Text: Guiding Language-Image Model via Image Tagging</a>\n",
    "            |\n",
    "            <a href='https://github.com/IDEA-Research/Grounded-Segment-Anything' target='_blank'>Grounded-Segment-Anything</a>\n",
    "        </p>\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    def inference_with_ram(img, do_det_seg):\n",
    "        return inference(\n",
    "            img, None, do_det_seg,\n",
    "            \"RAM\", ram_model, grounding_dino_model, sam_model\n",
    "        )\n",
    "\n",
    "    def inference_with_t2t(img, input_tags, do_det_seg):\n",
    "        return inference(\n",
    "            img, input_tags, do_det_seg,\n",
    "            \"Tag2Text\", tag2text_model, grounding_dino_model, sam_model\n",
    "        )\n",
    "\n",
    "    with gr.Blocks(title=\"Recognize Anything Model\") as demo:\n",
    "        ###############\n",
    "        # components\n",
    "        ###############\n",
    "        gr.HTML(description)\n",
    "\n",
    "        with gr.Tab(label=\"Recognize Anything Model\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    ram_in_img = gr.Image(type=\"pil\")\n",
    "                    ram_opt_det_seg = gr.Checkbox(label=\"Get Boxes and Masks with Grounded-SAM\", value=True)\n",
    "                    with gr.Row():\n",
    "                        ram_btn_run = gr.Button(value=\"Run\")\n",
    "                        try:\n",
    "                            ram_btn_clear = gr.ClearButton()\n",
    "                        except AttributeError:  # old gradio does not have ClearButton, not big problem\n",
    "                            ram_btn_clear = None\n",
    "                with gr.Column():\n",
    "                    ram_out_img = gr.Image(type=\"pil\")\n",
    "                    ram_out_tag = gr.Textbox(label=\"Tags\")\n",
    "                    ram_out_biaoqian = gr.Textbox(label=\"Ê†áÁ≠æ\")\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"images/demo/demo1.jpg\", True],\n",
    "                    [\"images/demo/demo2.jpg\", True],\n",
    "                    [\"images/demo/demo4.jpg\", True],\n",
    "                ],\n",
    "                fn=inference_with_ram,\n",
    "                inputs=[ram_in_img, ram_opt_det_seg],\n",
    "                outputs=[ram_out_tag, ram_out_biaoqian, ram_out_img],\n",
    "                cache_examples=True\n",
    "            )\n",
    "\n",
    "        with gr.Tab(label=\"Tag2Text Model\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    t2t_in_img = gr.Image(type=\"pil\")\n",
    "                    t2t_in_tag = gr.Textbox(label=\"User Specified Tags (Optional, separated by comma)\")\n",
    "                    t2t_opt_det_seg = gr.Checkbox(label=\"Get Boxes and Masks with Grounded-SAM\", value=True)\n",
    "                    with gr.Row():\n",
    "                        t2t_btn_run = gr.Button(value=\"Run\")\n",
    "                        try:\n",
    "                            t2t_btn_clear = gr.ClearButton()\n",
    "                        except AttributeError:  # old gradio does not have ClearButton, not big problem\n",
    "                            t2t_btn_clear = None\n",
    "                with gr.Column():\n",
    "                    t2t_out_img = gr.Image(type=\"pil\")\n",
    "                    t2t_out_tag = gr.Textbox(label=\"Tags\")\n",
    "                    t2t_out_cap = gr.Textbox(label=\"Caption\")\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"images/demo/demo4.jpg\", \"\", True],\n",
    "                    [\"images/demo/demo4.jpg\", \"power line\", False],\n",
    "                    [\"images/demo/demo4.jpg\", \"track, train\", False],\n",
    "                ],\n",
    "                fn=inference_with_t2t,\n",
    "                inputs=[t2t_in_img, t2t_in_tag, t2t_opt_det_seg],\n",
    "                outputs=[t2t_out_tag, t2t_out_cap, t2t_out_img],\n",
    "                cache_examples=True\n",
    "            )\n",
    "\n",
    "        gr.HTML(article)\n",
    "\n",
    "        ###############\n",
    "        # events\n",
    "        ###############\n",
    "        # run inference\n",
    "        ram_btn_run.click(\n",
    "            fn=inference_with_ram,\n",
    "            inputs=[ram_in_img, ram_opt_det_seg],\n",
    "            outputs=[ram_out_tag, ram_out_biaoqian, ram_out_img]\n",
    "        )\n",
    "        t2t_btn_run.click(\n",
    "            fn=inference_with_t2t,\n",
    "            inputs=[t2t_in_img, t2t_in_tag, t2t_opt_det_seg],\n",
    "            outputs=[t2t_out_tag, t2t_out_cap, t2t_out_img]\n",
    "        )\n",
    "\n",
    "        # hide or show image output\n",
    "        ram_opt_det_seg.change(fn=lambda b: gr.update(visible=b), inputs=[ram_opt_det_seg], outputs=[ram_out_img])\n",
    "        t2t_opt_det_seg.change(fn=lambda b: gr.update(visible=b), inputs=[t2t_opt_det_seg], outputs=[t2t_out_img])\n",
    "\n",
    "        # clear\n",
    "        if ram_btn_clear is not None:\n",
    "            ram_btn_clear.add([ram_in_img, ram_out_img, ram_out_tag, ram_out_biaoqian])\n",
    "        if t2t_btn_clear is not None:\n",
    "            t2t_btn_clear.add([t2t_in_img, t2t_in_tag, t2t_out_img, t2t_out_tag, t2t_out_cap])\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch demo\n",
    "build_gui().launch(\n",
    "    server_name=\"127.0.0.1\",  # localhost. use \"0.0.0.0\" to open to LAN\n",
    "    share=False  # use True to acquire a temporary public domain for sharing\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "523f14362536113a9aaf8abdd469f5064f3978ec857b89ec73e5ec5e937174a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
